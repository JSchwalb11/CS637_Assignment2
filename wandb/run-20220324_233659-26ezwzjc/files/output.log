Sequential(
  (0): Linear(in_features=18, out_features=25, bias=True)
  (1): ReLU()
  (2): Linear(in_features=25, out_features=34, bias=True)
  (3): Sigmoid()
  (4): Linear(in_features=34, out_features=4, bias=True)
  (5): ReLU()
  (6): Linear(in_features=4, out_features=1, bias=True)
)
Traceback (most recent call last):
  File "C:/Users/JoeyS/PycharmProjects/CS637_Assignment2/main.py", line 71, in <module>
    loss.backward()
  File "C:\Users\JoeyS\PycharmProjects\CS637_Assignment2\venv\lib\site-packages\torch\_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "C:\Users\JoeyS\PycharmProjects\CS637_Assignment2\venv\lib\site-packages\torch\autograd\__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "C:\Users\JoeyS\PycharmProjects\CS637_Assignment2\venv\lib\site-packages\wandb\wandb_torch.py", line 266, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
KeyboardInterrupt